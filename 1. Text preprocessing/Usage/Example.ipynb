{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tokenization, Lemmatization, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:25.635252Z",
     "start_time": "2021-09-08T12:59:25.628448Z"
    }
   },
   "outputs": [],
   "source": [
    "ru_text = \"\"\"\n",
    "–ü—Ä–∏–≤–µ—Ç, –º–µ–Ω—è –∑–æ–≤—É—Ç –¢—É—à–∏–Ω –ö–∏—Ä–∏–ª–ª,\n",
    "—è —Ä–æ–¥–∏–ª—Å—è 26.09.1997, –∑–∞–∫–æ–Ω—á–∏–ª –ú–§–¢–ò.\n",
    "–Ø –ø—ã—Ç–∞—é—Å—å –æ—Å–≤–æ–∏—Ç—å –∫—É—Ä—Å –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–∞!\n",
    "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω: 89876543210\n",
    "–¢–µ—Å—Ç–æ–≤–∞—è –ø–æ—á—Ç–∞: myemail@email.com\n",
    "–¢–µ—Å—Ç–æ–≤–∞—è —Å—É–º–º–∞ –¥–µ–Ω–µ–≥: 123456$\n",
    "üòé üî• ‚úåÔ∏è\n",
    "\"\"\"\n",
    "\n",
    "en_text = \"\"\"\n",
    "Hi, my name is Kirill Tushin,\n",
    "I was born on 26.09.1997, graduated from MIPT.\n",
    "I'm trying to master a course on text analysis!\n",
    "Test phone: 89876543210\n",
    "Test mail: myemail@email.com\n",
    "Test amount of money: 123456$\n",
    "üòé  üî•  ‚úåÔ∏è\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:25.646138Z",
     "start_time": "2021-09-08T12:59:25.640224Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_preprocess(tokens, prepared_tokens):\n",
    "    print(*[\n",
    "        f'Token: {token:10} Prepared: {prepare:10}'\n",
    "        for token, prepare in zip(tokens, prepared_tokens)\n",
    "    ], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T19:09:29.281563Z",
     "start_time": "2021-09-05T19:09:29.279457Z"
    }
   },
   "source": [
    "## Tokenization, Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:27.020078Z",
     "start_time": "2021-09-08T12:59:25.689683Z"
    }
   },
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:37.580391Z",
     "start_time": "2021-09-08T12:59:27.023102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c770a750b2418fb12d42001267b96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-08 15:59:27 INFO: Downloading default packages for language: ru (Russian)...\n",
      "2021-09-08 15:59:28 INFO: File exists: /Users/kirilltusin/stanza_resources/ru/default.zip.\n",
      "2021-09-08 15:59:34 INFO: Finished downloading models and saved to /Users/kirilltusin/stanza_resources.\n",
      "2021-09-08 15:59:34 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2021-09-08 15:59:34 INFO: Use device: cpu\n",
      "2021-09-08 15:59:34 INFO: Loading: tokenize\n",
      "2021-09-08 15:59:34 INFO: Loading: pos\n",
      "2021-09-08 15:59:34 INFO: Loading: lemma\n",
      "2021-09-08 15:59:34 INFO: Loading: depparse\n",
      "2021-09-08 15:59:35 INFO: Loading: ner\n",
      "2021-09-08 15:59:36 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: –ü—Ä–∏–≤–µ—Ç     Prepared: –ø—Ä–∏–≤–µ—Ç    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –º–µ–Ω—è       Prepared: —è         \n",
      "Token: –∑–æ–≤—É—Ç      Prepared: –∑–≤–∞—Ç—å     \n",
      "Token: –¢—É—à–∏–Ω      Prepared: –¢—É—à–∏–Ω     \n",
      "Token: –ö–∏—Ä–∏–ª–ª     Prepared: –ö–∏—Ä–∏–ª–ª    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: —è          Prepared: —è         \n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared: —Ä–æ–∂–¥–∞—Ç—å—Å—è \n",
      "Token: 26.09.1997 Prepared: 26.09.1997\n",
      "Token: ,          Prepared: ,         \n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared: –∑–∞–∫–æ–Ω—á–∏—Ç—å \n",
      "Token: –ú–§–¢–ò       Prepared: –ú–§–¢–ò      \n",
      "Token: .          Prepared: .         \n",
      "Token: –Ø          Prepared: —è         \n",
      "Token: –ø—ã—Ç–∞—é—Å—å    Prepared: –ø—ã—Ç–∞—Ç—å—Å—è  \n",
      "Token: –æ—Å–≤–æ–∏—Ç—å    Prepared: –æ—Å–≤–æ–∏—Ç—å   \n",
      "Token: –∫—É—Ä—Å       Prepared: –∫—É—Ä—Å      \n",
      "Token: –ø–æ         Prepared: –ø–æ        \n",
      "Token: –∞–Ω–∞–ª–∏–∑—É    Prepared: –∞–Ω–∞–ª–∏–∑    \n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared: —Ç–µ–∫—Å—Ç     \n",
      "Token: !          Prepared: !         \n",
      "Token: –¢–µ—Å—Ç–æ–≤—ã–π   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: —Ç–µ–ª–µ—Ñ–æ–Ω    Prepared: —Ç–µ–ª–µ—Ñ–æ–Ω   \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤–æ–π  \n",
      "Token: –ø–æ—á—Ç–∞      Prepared: –ø–æ—á—Ç–∞     \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail@email.com Prepared: myemail@email.com\n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤–æ–π  \n",
      "Token: —Å—É–º–º–∞      Prepared: —Å—É–º–º–∞     \n",
      "Token: –¥–µ–Ω–µ–≥      Prepared: –¥–µ–Ω—å–≥–∏    \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úåÔ∏è         Prepared: ‚úåÔ∏è        \n"
     ]
    }
   ],
   "source": [
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru')\n",
    "\n",
    "doc = nlp(ru_text)\n",
    "\n",
    "tokens = [\n",
    "    token.text\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.words\n",
    "]\n",
    "\n",
    "lemmas = [\n",
    "    token.lemma\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.words\n",
    "]\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:45.869495Z",
     "start_time": "2021-09-08T12:59:37.582834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e8b8e58b6140f490fa2ac1b2f9e830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-08 15:59:37 INFO: Downloading default packages for language: en (English)...\n",
      "2021-09-08 15:59:39 INFO: File exists: /Users/kirilltusin/stanza_resources/en/default.zip.\n",
      "2021-09-08 15:59:43 INFO: Finished downloading models and saved to /Users/kirilltusin/stanza_resources.\n",
      "2021-09-08 15:59:43 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-09-08 15:59:43 INFO: Use device: cpu\n",
      "2021-09-08 15:59:43 INFO: Loading: tokenize\n",
      "2021-09-08 15:59:43 INFO: Loading: pos\n",
      "2021-09-08 15:59:43 INFO: Loading: lemma\n",
      "2021-09-08 15:59:43 INFO: Loading: depparse\n",
      "2021-09-08 15:59:44 INFO: Loading: sentiment\n",
      "2021-09-08 15:59:44 INFO: Loading: ner\n",
      "2021-09-08 15:59:45 INFO: Done loading processors!\n",
      "/opt/miniconda3/envs/sber/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Hi         Prepared: hi        \n",
      "Token: ,          Prepared: ,         \n",
      "Token: my         Prepared: my        \n",
      "Token: name       Prepared: name      \n",
      "Token: is         Prepared: be        \n",
      "Token: Kirill     Prepared: Kirill    \n",
      "Token: Tushin     Prepared: Tushin    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: I          Prepared: I         \n",
      "Token: was        Prepared: be        \n",
      "Token: born       Prepared: bear      \n",
      "Token: on         Prepared: on        \n",
      "Token: 26.09.1997 Prepared: 26.09.1997\n",
      "Token: ,          Prepared: ,         \n",
      "Token: graduated  Prepared: graduate  \n",
      "Token: from       Prepared: from      \n",
      "Token: MIPT       Prepared: MIPT      \n",
      "Token: .          Prepared: .         \n",
      "Token: I          Prepared: I         \n",
      "Token: 'm         Prepared: be        \n",
      "Token: trying     Prepared: try       \n",
      "Token: to         Prepared: to        \n",
      "Token: master     Prepared: master    \n",
      "Token: a          Prepared: a         \n",
      "Token: course     Prepared: course    \n",
      "Token: on         Prepared: on        \n",
      "Token: text       Prepared: text      \n",
      "Token: analysis   Prepared: analysis  \n",
      "Token: !          Prepared: !         \n",
      "Token: Test       Prepared: test      \n",
      "Token: phone      Prepared: phone     \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: Test       Prepared: test      \n",
      "Token: mail       Prepared: mail      \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail@email.com Prepared: myemail@email.com\n",
      "Token: Test       Prepared: test      \n",
      "Token: amount     Prepared: amount    \n",
      "Token: of         Prepared: of        \n",
      "Token: money      Prepared: money     \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úåÔ∏è         Prepared: ‚úåÔ∏è        \n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "doc = nlp(en_text)\n",
    "\n",
    "tokens = [\n",
    "    token.text\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.words\n",
    "]\n",
    "\n",
    "lemmas = [\n",
    "    token.lemma\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.words\n",
    "]\n",
    "\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:47.066352Z",
     "start_time": "2021-09-08T12:59:45.874946Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:47.609647Z",
     "start_time": "2021-09-08T12:59:47.068856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: Hi         Prepared: hi        \n",
      "Token: ,          Prepared: ,         \n",
      "Token: my         Prepared: my        \n",
      "Token: name       Prepared: name      \n",
      "Token: is         Prepared: be        \n",
      "Token: Kirill     Prepared: Kirill    \n",
      "Token: Tushin     Prepared: Tushin    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: I          Prepared: I         \n",
      "Token: was        Prepared: be        \n",
      "Token: born       Prepared: bear      \n",
      "Token: on         Prepared: on        \n",
      "Token: 26.09.1997 Prepared: 26.09.1997\n",
      "Token: ,          Prepared: ,         \n",
      "Token: graduated  Prepared: graduate  \n",
      "Token: from       Prepared: from      \n",
      "Token: MIPT       Prepared: MIPT      \n",
      "Token: .          Prepared: .         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: I          Prepared: I         \n",
      "Token: 'm         Prepared: be        \n",
      "Token: trying     Prepared: try       \n",
      "Token: to         Prepared: to        \n",
      "Token: master     Prepared: master    \n",
      "Token: a          Prepared: a         \n",
      "Token: course     Prepared: course    \n",
      "Token: on         Prepared: on        \n",
      "Token: text       Prepared: text      \n",
      "Token: analysis   Prepared: analysis  \n",
      "Token: !          Prepared: !         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: Test       Prepared: test      \n",
      "Token: phone      Prepared: phone     \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: Test       Prepared: test      \n",
      "Token: mail       Prepared: mail      \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail@email.com Prepared: myemail@email.com\n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: Test       Prepared: test      \n",
      "Token: amount     Prepared: amount    \n",
      "Token: of         Prepared: of        \n",
      "Token: money      Prepared: money     \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token:            Prepared:           \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token:            Prepared:           \n",
      "Token: ‚úå          Prepared: ‚úå         \n",
      "Token: Ô∏è          Prepared: Ô∏è         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(en_text)\n",
    "tokens = [str(token) for token in doc]\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:49.460544Z",
     "start_time": "2021-09-08T12:59:47.611901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –ü—Ä–∏–≤–µ—Ç     Prepared: –ø—Ä–∏–≤–µ—Ç    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –º–µ–Ω—è       Prepared: –º–µ–Ω—è      \n",
      "Token: –∑–æ–≤—É—Ç      Prepared: –∑–≤–∞—Ç—å     \n",
      "Token: –¢—É—à–∏–Ω      Prepared: —Ç—É—à–∏–Ω     \n",
      "Token: –ö–∏—Ä–∏–ª–ª     Prepared: –∫–∏—Ä–∏–ª–ª    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: —è          Prepared: —è         \n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared: —Ä–æ–¥–∏—Ç—å—Å—è  \n",
      "Token: 26.09.1997 Prepared: 26.09.1997\n",
      "Token: ,          Prepared: ,         \n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared: –∑–∞–∫–æ–Ω—á–∏—Ç—å \n",
      "Token: –ú–§–¢–ò       Prepared: –º—Ñ—Ç–∏      \n",
      "Token: .          Prepared: .         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –Ø          Prepared: —è         \n",
      "Token: –ø—ã—Ç–∞—é—Å—å    Prepared: –ø—ã—Ç–∞—Ç—å—Å—è  \n",
      "Token: –æ—Å–≤–æ–∏—Ç—å    Prepared: –æ—Å–≤–æ–∏—Ç—å   \n",
      "Token: –∫—É—Ä—Å       Prepared: –∫—É—Ä—Å      \n",
      "Token: –ø–æ         Prepared: –ø–æ        \n",
      "Token: –∞–Ω–∞–ª–∏–∑—É    Prepared: –∞–Ω–∞–ª–∏–∑    \n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared: —Ç–µ–∫—Å—Ç     \n",
      "Token: !          Prepared: !         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –¢–µ—Å—Ç–æ–≤—ã–π   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: —Ç–µ–ª–µ—Ñ–æ–Ω    Prepared: —Ç–µ–ª–µ—Ñ–æ–Ω   \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: –ø–æ—á—Ç–∞      Prepared: –ø–æ—á—Ç–∞     \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail@email.com Prepared: myemail@email.com\n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: —Å—É–º–º–∞      Prepared: —Å—É–º–º–∞     \n",
      "Token: –¥–µ–Ω–µ–≥      Prepared: –¥–µ–Ω—å–≥–∞    \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úå          Prepared: ‚úå         \n",
      "Token: Ô∏è          Prepared: Ô∏è         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download ru_core_news_sm\n",
    "\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "doc = nlp(ru_text)\n",
    "tokens = [str(token) for token in doc]\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:50.635701Z",
     "start_time": "2021-09-08T12:59:49.465025Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:52.781207Z",
     "start_time": "2021-09-08T12:59:50.638159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Hi         Prepared: Hi        \n",
      "Token: my         Prepared: my        \n",
      "Token: name       Prepared: name      \n",
      "Token: is         Prepared: be        \n",
      "Token: Kirill     Prepared: Kirill    \n",
      "Token: Tushin     Prepared: Tushin    \n",
      "Token: I          Prepared: I         \n",
      "Token: was        Prepared: be        \n",
      "Token: born       Prepared: bear      \n",
      "Token: on         Prepared: on        \n",
      "Token: 26.09.1997 Prepared: 26.09.1997\n",
      "Token: graduated  Prepared: graduate  \n",
      "Token: from       Prepared: from      \n",
      "Token: MIPT       Prepared: MIPT      \n",
      "Token: I          Prepared: I         \n",
      "Token: 'm         Prepared: 'm        \n",
      "Token: trying     Prepared: try       \n",
      "Token: to         Prepared: to        \n",
      "Token: master     Prepared: master    \n",
      "Token: a          Prepared: a         \n",
      "Token: course     Prepared: course    \n",
      "Token: on         Prepared: on        \n",
      "Token: text       Prepared: text      \n",
      "Token: analysis   Prepared: analysis  \n",
      "Token: Test       Prepared: Test      \n",
      "Token: phone      Prepared: phone     \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: Test       Prepared: Test      \n",
      "Token: mail       Prepared: mail      \n",
      "Token: myemail    Prepared: myemail   \n",
      "Token: @          Prepared: @         \n",
      "Token: email.com  Prepared: email.com \n",
      "Token: Test       Prepared: Test      \n",
      "Token: amount     Prepared: amount    \n",
      "Token: of         Prepared: of        \n",
      "Token: money      Prepared: money     \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úåÔ∏è         Prepared: ‚úåÔ∏è        \n"
     ]
    }
   ],
   "source": [
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\n",
    "        'J': 'a', \n",
    "        'N': 'n', \n",
    "        'V': 'v', \n",
    "        'R': 'r',\n",
    "    }\n",
    "    tokens_and_tags = [\n",
    "        (token, tag_dict.get(pos[0], 'n'))\n",
    "        for token, pos in sent.tags\n",
    "    ]\n",
    "    tokens = [token for token, pos in sent.tags]\n",
    "    lemmas = [token.lemmatize(tag) for token, tag in tokens_and_tags]\n",
    "    return tokens, lemmas\n",
    "\n",
    "tokens, lemmas = lemmatize_with_postag(en_text)\n",
    "\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:52.818356Z",
     "start_time": "2021-09-08T12:59:52.804116Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from pymorphy2.tokenizers import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:53.126466Z",
     "start_time": "2021-09-08T12:59:52.837757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: –ü—Ä–∏–≤–µ—Ç     Prepared: –ø—Ä–∏–≤–µ—Ç    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –º–µ–Ω—è       Prepared: —è         \n",
      "Token: –∑–æ–≤—É—Ç      Prepared: –∑–≤–∞—Ç—å     \n",
      "Token: –¢—É—à–∏–Ω      Prepared: —Ç—É—à–∏–Ω     \n",
      "Token: –ö–∏—Ä–∏–ª–ª     Prepared: –∫–∏—Ä–∏–ª–ª    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: —è          Prepared: —è         \n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared: —Ä–æ–¥–∏—Ç—å—Å—è  \n",
      "Token: 26         Prepared: 26        \n",
      "Token: .          Prepared: .         \n",
      "Token: 09         Prepared: 09        \n",
      "Token: .          Prepared: .         \n",
      "Token: 1997       Prepared: 1997      \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared: –∑–∞–∫–æ–Ω—á–∏—Ç—å \n",
      "Token: –ú–§–¢–ò       Prepared: –º—Ñ—Ç—å      \n",
      "Token: .          Prepared: .         \n",
      "Token: –Ø          Prepared: —è         \n",
      "Token: –ø—ã—Ç–∞—é—Å—å    Prepared: –ø—ã—Ç–∞—Ç—å—Å—è  \n",
      "Token: –æ—Å–≤–æ–∏—Ç—å    Prepared: –æ—Å–≤–æ–∏—Ç—å   \n",
      "Token: –∫—É—Ä—Å       Prepared: –∫—É—Ä—Å      \n",
      "Token: –ø–æ         Prepared: –ø–æ        \n",
      "Token: –∞–Ω–∞–ª–∏–∑—É    Prepared: –∞–Ω–∞–ª–∏–∑    \n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared: —Ç–µ–∫—Å—Ç     \n",
      "Token: !          Prepared: !         \n",
      "Token: –¢–µ—Å—Ç–æ–≤—ã–π   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: —Ç–µ–ª–µ—Ñ–æ–Ω    Prepared: —Ç–µ–ª–µ—Ñ–æ–Ω   \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: –ø–æ—á—Ç–∞      Prepared: –ø–æ—á—Ç–∞     \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail    Prepared: myemail   \n",
      "Token: @          Prepared: @         \n",
      "Token: email      Prepared: email     \n",
      "Token: .          Prepared: .         \n",
      "Token: com        Prepared: com       \n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: —Å—É–º–º–∞      Prepared: —Å—É–º–º–∞     \n",
      "Token: –¥–µ–Ω–µ–≥      Prepared: –¥–µ–Ω—å–≥–∞    \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úå          Prepared: ‚úå         \n",
      "Token: Ô∏è          Prepared: Ô∏è         \n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='ru')\n",
    "tokens = simple_word_tokenize(ru_text)\n",
    "lemmas = [\n",
    "    morph.parse(token)[0].normal_form\n",
    "    for token in tokens\n",
    "]\n",
    "\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:53.149761Z",
     "start_time": "2021-09-08T12:59:53.134176Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:54.714732Z",
     "start_time": "2021-09-08T12:59:53.152464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –ü—Ä–∏–≤–µ—Ç     Prepared: –ø—Ä–∏–≤–µ—Ç    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –º–µ–Ω—è       Prepared: —è         \n",
      "Token:            Prepared:           \n",
      "Token: –∑–æ–≤—É—Ç      Prepared: –∑–≤–∞—Ç—å     \n",
      "Token:            Prepared:           \n",
      "Token: –¢—É—à–∏–Ω      Prepared: —Ç—É—à–∏–Ω     \n",
      "Token:            Prepared:           \n",
      "Token: –ö–∏—Ä–∏–ª–ª     Prepared: –∫–∏—Ä–∏–ª–ª    \n",
      "Token: ,\n",
      "         Prepared: ,\n",
      "        \n",
      "Token: —è          Prepared: —è         \n",
      "Token:            Prepared:           \n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared: —Ä–æ–∂–¥–∞—Ç—å—Å—è \n",
      "Token:            Prepared:           \n",
      "Token: 26.09      Prepared: 26.09     \n",
      "Token: .          Prepared: .         \n",
      "Token: 1997       Prepared: 1997      \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared: –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å\n",
      "Token:            Prepared:           \n",
      "Token: –ú–§–¢–ò       Prepared: –º—Ñ—Ç–∏      \n",
      "Token: .          Prepared: .         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –Ø          Prepared: —è         \n",
      "Token:            Prepared:           \n",
      "Token: –ø—ã—Ç–∞—é—Å—å    Prepared: –ø—ã—Ç–∞—Ç—å—Å—è  \n",
      "Token:            Prepared:           \n",
      "Token: –æ—Å–≤–æ–∏—Ç—å    Prepared: –æ—Å–≤–∞–∏–≤–∞—Ç—å \n",
      "Token:            Prepared:           \n",
      "Token: –∫—É—Ä—Å       Prepared: –∫—É—Ä—Å      \n",
      "Token:            Prepared:           \n",
      "Token: –ø–æ         Prepared: –ø–æ        \n",
      "Token:            Prepared:           \n",
      "Token: –∞–Ω–∞–ª–∏–∑—É    Prepared: –∞–Ω–∞–ª–∏–∑    \n",
      "Token:            Prepared:           \n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared: —Ç–µ–∫—Å—Ç     \n",
      "Token: !          Prepared: !         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –¢–µ—Å—Ç–æ–≤—ã–π   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token:            Prepared:           \n",
      "Token: —Ç–µ–ª–µ—Ñ–æ–Ω    Prepared: —Ç–µ–ª–µ—Ñ–æ–Ω   \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token:            Prepared:           \n",
      "Token: –ø–æ—á—Ç–∞      Prepared: –ø–æ—á—Ç–∞     \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail    Prepared: myemail   \n",
      "Token: @          Prepared: @         \n",
      "Token: email      Prepared: email     \n",
      "Token: .          Prepared: .         \n",
      "Token: com        Prepared: com       \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token:            Prepared:           \n",
      "Token: —Å—É–º–º–∞      Prepared: —Å—É–º–º–∞     \n",
      "Token:            Prepared:           \n",
      "Token: –¥–µ–Ω–µ–≥      Prepared: –¥–µ–Ω—å–≥–∏    \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token:            Prepared:           \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token:            Prepared:           \n",
      "Token: ‚úå          Prepared: ‚úå         \n"
     ]
    }
   ],
   "source": [
    "m = Mystem()\n",
    "tokens = [x['text'] for x in m.analyze(ru_text)][:-1]\n",
    "lemmas = m.lemmatize(ru_text)[:-1]\n",
    "\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:55.960926Z",
     "start_time": "2021-09-08T12:59:54.718194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: Hi         Prepared: Hi        \n",
      "Token: ,          Prepared: ,         \n",
      "Token: my         Prepared: my        \n",
      "Token:            Prepared:           \n",
      "Token: name       Prepared: name      \n",
      "Token:            Prepared:           \n",
      "Token: is         Prepared: is        \n",
      "Token:            Prepared:           \n",
      "Token: Kirill     Prepared: Kirill    \n",
      "Token:            Prepared:           \n",
      "Token: Tushin     Prepared: Tushin    \n",
      "Token: ,\n",
      "         Prepared: ,\n",
      "        \n",
      "Token: I          Prepared: I         \n",
      "Token:            Prepared:           \n",
      "Token: was        Prepared: was       \n",
      "Token:            Prepared:           \n",
      "Token: born       Prepared: born      \n",
      "Token:            Prepared:           \n",
      "Token: on         Prepared: on        \n",
      "Token:            Prepared:           \n",
      "Token: 26.09      Prepared: 26.09     \n",
      "Token: .          Prepared: .         \n",
      "Token: 1997       Prepared: 1997      \n",
      "Token: ,          Prepared: ,         \n",
      "Token: graduated  Prepared: graduated \n",
      "Token:            Prepared:           \n",
      "Token: from       Prepared: from      \n",
      "Token:            Prepared:           \n",
      "Token: MIPT       Prepared: MIPT      \n",
      "Token: .          Prepared: .         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: I          Prepared: I         \n",
      "Token: '          Prepared: '         \n",
      "Token: m          Prepared: m         \n",
      "Token:            Prepared:           \n",
      "Token: trying     Prepared: trying    \n",
      "Token:            Prepared:           \n",
      "Token: to         Prepared: to        \n",
      "Token:            Prepared:           \n",
      "Token: master     Prepared: master    \n",
      "Token:            Prepared:           \n",
      "Token: a          Prepared: a         \n",
      "Token:            Prepared:           \n",
      "Token: course     Prepared: course    \n",
      "Token:            Prepared:           \n",
      "Token: on         Prepared: on        \n",
      "Token:            Prepared:           \n",
      "Token: text       Prepared: text      \n",
      "Token:            Prepared:           \n",
      "Token: analysis   Prepared: analysis  \n",
      "Token: !          Prepared: !         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: Test       Prepared: Test      \n",
      "Token:            Prepared:           \n",
      "Token: phone      Prepared: phone     \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: Test       Prepared: Test      \n",
      "Token:            Prepared:           \n",
      "Token: mail       Prepared: mail      \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail    Prepared: myemail   \n",
      "Token: @          Prepared: @         \n",
      "Token: email      Prepared: email     \n",
      "Token: .          Prepared: .         \n",
      "Token: com        Prepared: com       \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: Test       Prepared: Test      \n",
      "Token:            Prepared:           \n",
      "Token: amount     Prepared: amount    \n",
      "Token:            Prepared:           \n",
      "Token: of         Prepared: of        \n",
      "Token:            Prepared:           \n",
      "Token: money      Prepared: money     \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: \n",
      "          Prepared: \n",
      "         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token:            Prepared:           \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token:            Prepared:           \n",
      "Token: ‚úå          Prepared: ‚úå         \n"
     ]
    }
   ],
   "source": [
    "m = Mystem()\n",
    "tokens = [x['text'] for x in m.analyze(en_text)][:-1]\n",
    "lemmas = m.lemmatize(en_text)[:-1]\n",
    "\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:56.837263Z",
     "start_time": "2021-09-08T12:59:55.964632Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: –ü—Ä–∏–≤–µ—Ç     Prepared: –ø—Ä–∏–≤–µ—Ç    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –º–µ–Ω—è       Prepared: —è         \n",
      "Token: –∑–æ–≤—É—Ç      Prepared: –∑–≤–∞—Ç—å     \n",
      "Token: –¢—É—à–∏–Ω      Prepared: —Ç—É—à–∏–Ω     \n",
      "Token: –ö–∏—Ä–∏–ª–ª     Prepared: –∫–∏—Ä–∏–ª–ª    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: —è          Prepared: —è         \n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared: —Ä–æ–¥–∏—Ç—å—Å—è  \n",
      "Token: 26.09.1997 Prepared: 26.09.1997\n",
      "Token: ,          Prepared: ,         \n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared: –∑–∞–∫–æ–Ω—á–∏—Ç—å \n",
      "Token: –ú–§–¢–ò       Prepared: –º—Ñ—Ç—å      \n",
      "Token: .          Prepared: .         \n",
      "Token: –Ø          Prepared: —è         \n",
      "Token: –ø—ã—Ç–∞—é—Å—å    Prepared: –ø—ã—Ç–∞—Ç—å—Å—è  \n",
      "Token: –æ—Å–≤–æ–∏—Ç—å    Prepared: –æ—Å–≤–æ–∏—Ç—å   \n",
      "Token: –∫—É—Ä—Å       Prepared: –∫—É—Ä—Å      \n",
      "Token: –ø–æ         Prepared: –ø–æ        \n",
      "Token: –∞–Ω–∞–ª–∏–∑—É    Prepared: –∞–Ω–∞–ª–∏–∑    \n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared: —Ç–µ–∫—Å—Ç     \n",
      "Token: !          Prepared: !         \n",
      "Token: –¢–µ—Å—Ç–æ–≤—ã–π   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: —Ç–µ–ª–µ—Ñ–æ–Ω    Prepared: —Ç–µ–ª–µ—Ñ–æ–Ω   \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: –ø–æ—á—Ç–∞      Prepared: –ø–æ—á—Ç–∞     \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail    Prepared: myemail   \n",
      "Token: @          Prepared: @         \n",
      "Token: email      Prepared: email     \n",
      "Token: .          Prepared: .         \n",
      "Token: com        Prepared: com       \n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤—ã–π  \n",
      "Token: —Å—É–º–º–∞      Prepared: —Å—É–º–º–∞     \n",
      "Token: –¥–µ–Ω–µ–≥      Prepared: –¥–µ–Ω—å–≥–∞    \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úåÔ∏è         Prepared: ‚úåÔ∏è        \n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Doc,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Segmenter,\n",
    ")\n",
    "\n",
    "morph_vocab = MorphVocab()\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "doc = Doc(ru_text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "\n",
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "    \n",
    "tokens = [token.text for token in doc.tokens]\n",
    "lemmas = [token.lemma for token in doc.tokens]\n",
    "\n",
    "print_preprocess(tokens, lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization, Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:56.843955Z",
     "start_time": "2021-09-08T12:59:56.840649Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:56.858434Z",
     "start_time": "2021-09-08T12:59:56.847506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Hi         Prepared: hi        \n",
      "Token: ,          Prepared: ,         \n",
      "Token: my         Prepared: my        \n",
      "Token: name       Prepared: name      \n",
      "Token: is         Prepared: is        \n",
      "Token: Kirill     Prepared: kiril     \n",
      "Token: Tushin     Prepared: tushin    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: I          Prepared: i         \n",
      "Token: was        Prepared: was       \n",
      "Token: born       Prepared: born      \n",
      "Token: on         Prepared: on        \n",
      "Token: 26         Prepared: 26        \n",
      "Token: .          Prepared: .         \n",
      "Token: 09         Prepared: 09        \n",
      "Token: .          Prepared: .         \n",
      "Token: 1997       Prepared: 1997      \n",
      "Token: ,          Prepared: ,         \n",
      "Token: graduated  Prepared: graduat   \n",
      "Token: from       Prepared: from      \n",
      "Token: MIPT       Prepared: mipt      \n",
      "Token: .          Prepared: .         \n",
      "Token: I          Prepared: i         \n",
      "Token: '          Prepared: '         \n",
      "Token: m          Prepared: m         \n",
      "Token: trying     Prepared: tri       \n",
      "Token: to         Prepared: to        \n",
      "Token: master     Prepared: master    \n",
      "Token: a          Prepared: a         \n",
      "Token: course     Prepared: cours     \n",
      "Token: on         Prepared: on        \n",
      "Token: text       Prepared: text      \n",
      "Token: analysis   Prepared: analysi   \n",
      "Token: !          Prepared: !         \n",
      "Token: Test       Prepared: test      \n",
      "Token: phone      Prepared: phone     \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: Test       Prepared: test      \n",
      "Token: mail       Prepared: mail      \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail    Prepared: myemail   \n",
      "Token: @          Prepared: @         \n",
      "Token: email      Prepared: email     \n",
      "Token: .          Prepared: .         \n",
      "Token: com        Prepared: com       \n",
      "Token: Test       Prepared: test      \n",
      "Token: amount     Prepared: amount    \n",
      "Token: of         Prepared: of        \n",
      "Token: money      Prepared: money     \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úåÔ∏è         Prepared: ‚úåÔ∏è        \n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(en_text)\n",
    "stemms = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print_preprocess(tokens, stemms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:56.872985Z",
     "start_time": "2021-09-08T12:59:56.862586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: –ü—Ä–∏–≤–µ—Ç     Prepared: –ø—Ä–∏–≤–µ—Ç    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –º–µ–Ω—è       Prepared: –º–µ–Ω       \n",
      "Token: –∑–æ–≤—É—Ç      Prepared: –∑–æ–≤—É—Ç     \n",
      "Token: –¢—É—à–∏–Ω      Prepared: —Ç—É—à–∏–Ω     \n",
      "Token: –ö–∏—Ä–∏–ª–ª     Prepared: –∫–∏—Ä–∏–ª–ª    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: —è          Prepared: —è         \n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared: —Ä–æ–¥       \n",
      "Token: 26         Prepared: 26        \n",
      "Token: .          Prepared: .         \n",
      "Token: 09         Prepared: 09        \n",
      "Token: .          Prepared: .         \n",
      "Token: 1997       Prepared: 1997      \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared: –∑–∞–∫–æ–Ω—á    \n",
      "Token: –ú–§–¢–ò       Prepared: –º—Ñ—Ç–∏      \n",
      "Token: .          Prepared: .         \n",
      "Token: –Ø          Prepared: —è         \n",
      "Token: –ø—ã—Ç–∞—é—Å—å    Prepared: –ø—ã—Ç–∞      \n",
      "Token: –æ—Å–≤–æ–∏—Ç—å    Prepared: –æ—Å–≤–æ      \n",
      "Token: –∫—É—Ä—Å       Prepared: –∫—É—Ä—Å      \n",
      "Token: –ø–æ         Prepared: –ø–æ        \n",
      "Token: –∞–Ω–∞–ª–∏–∑—É    Prepared: –∞–Ω–∞–ª–∏–∑    \n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared: —Ç–µ–∫—Å—Ç     \n",
      "Token: !          Prepared: !         \n",
      "Token: –¢–µ—Å—Ç–æ–≤—ã–π   Prepared: —Ç–µ—Å—Ç–æ–≤    \n",
      "Token: —Ç–µ–ª–µ—Ñ–æ–Ω    Prepared: —Ç–µ–ª–µ—Ñ–æ–Ω   \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤    \n",
      "Token: –ø–æ—á—Ç–∞      Prepared: –ø–æ—á—Ç      \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail    Prepared: myemail   \n",
      "Token: @          Prepared: @         \n",
      "Token: email      Prepared: email     \n",
      "Token: .          Prepared: .         \n",
      "Token: com        Prepared: com       \n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤    \n",
      "Token: —Å—É–º–º–∞      Prepared: —Å—É–º–º      \n",
      "Token: –¥–µ–Ω–µ–≥      Prepared: –¥–µ–Ω–µ–≥     \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úåÔ∏è         Prepared: ‚úåÔ∏è        \n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('russian')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(ru_text)\n",
    "stemms = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print_preprocess(tokens, stemms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:56.887064Z",
     "start_time": "2021-09-08T12:59:56.875815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: –ü—Ä–∏–≤–µ—Ç     Prepared: –ø—Ä–∏–≤–µ—Ç    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –º–µ–Ω—è       Prepared: –º–µ–Ω       \n",
      "Token: –∑–æ–≤—É—Ç      Prepared: –∑–æ–≤—É—Ç     \n",
      "Token: –¢—É—à–∏–Ω      Prepared: —Ç—É—à–∏–Ω     \n",
      "Token: –ö–∏—Ä–∏–ª–ª     Prepared: –∫–∏—Ä–∏–ª–ª    \n",
      "Token: ,          Prepared: ,         \n",
      "Token: —è          Prepared: —è         \n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared: —Ä–æ–¥       \n",
      "Token: 26         Prepared: 26        \n",
      "Token: .          Prepared: .         \n",
      "Token: 09         Prepared: 09        \n",
      "Token: .          Prepared: .         \n",
      "Token: 1997       Prepared: 1997      \n",
      "Token: ,          Prepared: ,         \n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared: –∑–∞–∫–æ–Ω—á    \n",
      "Token: –ú–§–¢–ò       Prepared: –º—Ñ—Ç–∏      \n",
      "Token: .          Prepared: .         \n",
      "Token: –Ø          Prepared: —è         \n",
      "Token: –ø—ã—Ç–∞—é—Å—å    Prepared: –ø—ã—Ç–∞      \n",
      "Token: –æ—Å–≤–æ–∏—Ç—å    Prepared: –æ—Å–≤–æ      \n",
      "Token: –∫—É—Ä—Å       Prepared: –∫—É—Ä—Å      \n",
      "Token: –ø–æ         Prepared: –ø–æ        \n",
      "Token: –∞–Ω–∞–ª–∏–∑—É    Prepared: –∞–Ω–∞–ª–∏–∑    \n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared: —Ç–µ–∫—Å—Ç     \n",
      "Token: !          Prepared: !         \n",
      "Token: –¢–µ—Å—Ç–æ–≤—ã–π   Prepared: —Ç–µ—Å—Ç–æ–≤    \n",
      "Token: —Ç–µ–ª–µ—Ñ–æ–Ω    Prepared: —Ç–µ–ª–µ—Ñ–æ–Ω   \n",
      "Token: :          Prepared: :         \n",
      "Token: 89876543210 Prepared: 89876543210\n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤    \n",
      "Token: –ø–æ—á—Ç–∞      Prepared: –ø–æ—á—Ç      \n",
      "Token: :          Prepared: :         \n",
      "Token: myemail    Prepared: myemail   \n",
      "Token: @          Prepared: @         \n",
      "Token: email      Prepared: email     \n",
      "Token: .          Prepared: .         \n",
      "Token: com        Prepared: com       \n",
      "Token: –¢–µ—Å—Ç–æ–≤–∞—è   Prepared: —Ç–µ—Å—Ç–æ–≤    \n",
      "Token: —Å—É–º–º–∞      Prepared: —Å—É–º–º      \n",
      "Token: –¥–µ–Ω–µ–≥      Prepared: –¥–µ–Ω–µ–≥     \n",
      "Token: :          Prepared: :         \n",
      "Token: 123456     Prepared: 123456    \n",
      "Token: $          Prepared: $         \n",
      "Token: üòé          Prepared: üòé         \n",
      "Token: üî•          Prepared: üî•         \n",
      "Token: ‚úåÔ∏è         Prepared: ‚úåÔ∏è        \n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.snowball.RussianStemmer()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(ru_text)\n",
    "stemms = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print_preprocess(tokens, stemms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T10:21:13.821442Z",
     "start_time": "2021-09-08T10:21:13.821419Z"
    }
   },
   "source": [
    "# Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T12:59:56.958208Z",
     "start_time": "2021-09-08T12:59:56.890735Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "    dataset = [string.strip() for string in dataset]\n",
    "    dataset = [string for string in dataset if string]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "with open('../../data/war_and_peace_ru.txt', 'r') as ru_dataset_file:\n",
    "    ru_dataset = ru_dataset_file.readlines()\n",
    "    ru_dataset = clean_dataset(ru_dataset)\n",
    "    \n",
    "with open('../../data/the_picture_of_dorian_gray.txt', 'r') as en_dataset_file:\n",
    "    en_dataset = en_dataset_file.readlines()\n",
    "    en_dataset = clean_dataset(en_dataset)\n",
    "\n",
    "\n",
    "full_dataset = ru_dataset + en_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:00.030404Z",
     "start_time": "2021-09-08T12:59:56.961165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "\n",
    "vocab_size = 50000\n",
    "dropout = 0.1\n",
    "max_length = 64\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(BPE(dropout=dropout))\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=['<sos>', '<pad>', '<eos>'],\n",
    ")\n",
    "\n",
    "### Train on full dataset\n",
    "tokenizer.train_from_iterator(full_dataset, trainer)\n",
    "\n",
    "\n",
    "### Add special tokens like in BERT model\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "    ('<eos>', tokenizer.token_to_id('<eos>')),\n",
    "    ('<sos>', tokenizer.token_to_id('<sos>')),\n",
    ")\n",
    "\n",
    "### Enable padding\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=tokenizer.token_to_id('<pad>'),\n",
    "    length=max_length,\n",
    ")\n",
    "\n",
    "### Enable truncation\n",
    "tokenizer.enable_truncation(\n",
    "    max_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:00.117956Z",
     "start_time": "2021-09-08T13:00:00.078089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=64, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(ru_text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:00.139432Z",
     "start_time": "2021-09-08T13:00:00.121110Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: <sos>      Prepared:          0\n",
      "Token: –ü—Ä–∏        Prepared:       1634\n",
      "Token: –≤–µ—Ç        Prepared:       2118\n",
      "Token: ,          Prepared:         10\n",
      "Token: –º–µ–Ω—è       Prepared:        565\n",
      "Token: –∑–æ–≤—É—Ç      Prepared:      13097\n",
      "Token: –¢—É—à–∏–Ω      Prepared:       2958\n",
      "Token: –ö–∏—Ä–∏–ª      Prepared:       6176\n",
      "Token: –ª          Prepared:        133\n",
      "Token: ,          Prepared:         10\n",
      "Token: —è          Prepared:        153\n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared:      19205\n",
      "Token: 26         Prepared:      29993\n",
      "Token: .          Prepared:         12\n",
      "Token: 0          Prepared:         13\n",
      "Token: 9          Prepared:         22\n",
      "Token: .          Prepared:         12\n",
      "Token: 19         Prepared:       9981\n",
      "Token: 9          Prepared:         22\n",
      "Token: 7          Prepared:         20\n",
      "Token: ,          Prepared:         10\n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared:      45128\n",
      "Token: –ú          Prepared:        104\n",
      "Token: –§          Prepared:        112\n",
      "Token: –¢          Prepared:        110\n",
      "Token: –ò          Prepared:        100\n",
      "Token: .          Prepared:         12\n",
      "Token: –Ø          Prepared:        121\n",
      "Token: –ø—ã         Prepared:        731\n",
      "Token: —Ç–∞         Prepared:        193\n",
      "Token: —é—Å—å        Prepared:       1585\n",
      "Token: –æ          Prepared:        136\n",
      "Token: —Å–≤–æ–∏       Prepared:        907\n",
      "Token: —Ç—å         Prepared:        179\n",
      "Token: –∫—É—Ä        Prepared:       4919\n",
      "Token: —Å          Prepared:        139\n",
      "Token: –ø–æ         Prepared:        168\n",
      "Token: –∞–Ω–∞–ª–∏      Prepared:      31414\n",
      "Token: –∑—É         Prepared:       1303\n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared:      14079\n",
      "Token: !          Prepared:          3\n",
      "Token: –¢–µ         Prepared:       1769\n",
      "Token: —Å—Ç–æ        Prepared:        232\n",
      "Token: –≤—ã–π        Prepared:        801\n",
      "Token: —Ç–µ–ª–µ       Prepared:       8667\n",
      "Token: —Ñ–æ–Ω        Prepared:       9241\n",
      "Token: :          Prepared:         23\n",
      "Token: 8          Prepared:         21\n",
      "Token: 9          Prepared:         22\n",
      "Token: 8          Prepared:         21\n",
      "Token: 7          Prepared:         20\n",
      "Token: 6          Prepared:         19\n",
      "Token: 5          Prepared:         18\n",
      "Token: 43         Prepared:      14290\n",
      "Token: 2          Prepared:         15\n",
      "Token: 10         Prepared:       3713\n",
      "Token: –¢–µ         Prepared:       1769\n",
      "Token: —Å—Ç–æ        Prepared:        232\n",
      "Token: –≤–∞—è        Prepared:        381\n",
      "Token: –ø–æ—á        Prepared:        888\n",
      "Token: —Ç–∞         Prepared:        193\n",
      "Token: :          Prepared:         23\n",
      "Token: my         Prepared:        640\n",
      "Token: <eos>      Prepared:          2\n"
     ]
    }
   ],
   "source": [
    "print_preprocess(encoding.tokens, encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:00.178248Z",
     "start_time": "2021-09-08T13:00:00.155577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=64, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(en_text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:00.218902Z",
     "start_time": "2021-09-08T13:00:00.193565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: <sos>      Prepared:          0\n",
      "Token: Hi         Prepared:      21290\n",
      "Token: ,          Prepared:         10\n",
      "Token: my         Prepared:        640\n",
      "Token: name       Prepared:       2598\n",
      "Token: i          Prepared:         63\n",
      "Token: s          Prepared:         73\n",
      "Token: K          Prepared:         36\n",
      "Token: i          Prepared:         63\n",
      "Token: rill       Prepared:       5114\n",
      "Token: T          Prepared:         45\n",
      "Token: ush        Prepared:       3358\n",
      "Token: in         Prepared:        178\n",
      "Token: ,          Prepared:         10\n",
      "Token: I          Prepared:         34\n",
      "Token: was        Prepared:        320\n",
      "Token: bor        Prepared:       2263\n",
      "Token: n          Prepared:         68\n",
      "Token: on         Prepared:        204\n",
      "Token: 26         Prepared:      29993\n",
      "Token: .          Prepared:         12\n",
      "Token: 0          Prepared:         13\n",
      "Token: 9          Prepared:         22\n",
      "Token: .          Prepared:         12\n",
      "Token: 1          Prepared:         14\n",
      "Token: 9          Prepared:         22\n",
      "Token: 9          Prepared:         22\n",
      "Token: 7          Prepared:         20\n",
      "Token: ,          Prepared:         10\n",
      "Token: g          Prepared:         61\n",
      "Token: ra         Prepared:        346\n",
      "Token: du         Prepared:       1179\n",
      "Token: ated       Prepared:       1624\n",
      "Token: from       Prepared:        798\n",
      "Token: M          Prepared:         38\n",
      "Token: I          Prepared:         34\n",
      "Token: P          Prepared:         41\n",
      "Token: T          Prepared:         45\n",
      "Token: .          Prepared:         12\n",
      "Token: I          Prepared:         34\n",
      "Token: '          Prepared:          6\n",
      "Token: m          Prepared:         67\n",
      "Token: trying     Prepared:       7261\n",
      "Token: to         Prepared:        229\n",
      "Token: master     Prepared:       7831\n",
      "Token: a          Prepared:         55\n",
      "Token: c          Prepared:         57\n",
      "Token: our        Prepared:        545\n",
      "Token: se         Prepared:        270\n",
      "Token: on         Prepared:        204\n",
      "Token: te         Prepared:        433\n",
      "Token: xt         Prepared:       4424\n",
      "Token: analy      Prepared:      14694\n",
      "Token: sis        Prepared:      30830\n",
      "Token: !          Prepared:          3\n",
      "Token: T          Prepared:         45\n",
      "Token: est        Prepared:        596\n",
      "Token: ph         Prepared:       3819\n",
      "Token: one        Prepared:        464\n",
      "Token: :          Prepared:         23\n",
      "Token: 8          Prepared:         21\n",
      "Token: 9          Prepared:         22\n",
      "Token: 8          Prepared:         21\n",
      "Token: <eos>      Prepared:          2\n"
     ]
    }
   ],
   "source": [
    "print_preprocess(encoding.tokens, encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:00.227152Z",
     "start_time": "2021-09-08T13:00:00.223383Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import CharBPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:03.447435Z",
     "start_time": "2021-09-08T13:00:00.246408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharBPETokenizer()\n",
    "tokenizer.train_from_iterator(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:03.456602Z",
     "start_time": "2021-09-08T13:00:03.450248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=90, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(ru_text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:03.475551Z",
     "start_time": "2021-09-08T13:00:03.467753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: –ü—Ä–∏        Prepared:       2157\n",
      "Token: –≤–µ—Ç</w>    Prepared:       4308\n",
      "Token: ,</w>      Prepared:        275\n",
      "Token: –º–µ–Ω—è</w>   Prepared:        768\n",
      "Token: –∑–æ–≤—É—Ç</w>  Prepared:      14019\n",
      "Token: –¢—É—à–∏–Ω</w>  Prepared:       3356\n",
      "Token: –ö–∏—Ä–∏–ª–ª</w> Prepared:      17622\n",
      "Token: ,</w>      Prepared:        275\n",
      "Token: —è</w>      Prepared:        181\n",
      "Token: —Ä–æ–¥–∏–ª—Å—è</w> Prepared:      20447\n",
      "Token: 2          Prepared:         13\n",
      "Token: 6</w>      Prepared:        226\n",
      "Token: .</w>      Prepared:        259\n",
      "Token: 0          Prepared:         11\n",
      "Token: 9</w>      Prepared:        214\n",
      "Token: .</w>      Prepared:        259\n",
      "Token: 1          Prepared:         12\n",
      "Token: 9          Prepared:         20\n",
      "Token: 9          Prepared:         20\n",
      "Token: 7</w>      Prepared:        218\n",
      "Token: ,</w>      Prepared:        275\n",
      "Token: –∑–∞–∫–æ–Ω      Prepared:       5056\n",
      "Token: —á–∏–ª</w>    Prepared:       1854\n",
      "Token: –ú          Prepared:        102\n",
      "Token: –§          Prepared:        110\n",
      "Token: –¢          Prepared:        108\n",
      "Token: –ò</w>      Prepared:        213\n",
      "Token: .</w>      Prepared:        259\n",
      "Token: –Ø</w>      Prepared:        207\n",
      "Token: –ø—ã         Prepared:       1018\n",
      "Token: —Ç–∞         Prepared:        323\n",
      "Token: —é—Å—å</w>    Prepared:       1739\n",
      "Token: –æ          Prepared:        134\n",
      "Token: —Å–≤–æ        Prepared:        388\n",
      "Token: –∏          Prepared:        128\n",
      "Token: —Ç—å</w>     Prepared:        307\n",
      "Token: –∫—É—Ä        Prepared:       4937\n",
      "Token: —Å</w>      Prepared:        201\n",
      "Token: –ø–æ</w>     Prepared:        500\n",
      "Token: –∞–Ω         Prepared:       1328\n",
      "Token: –∞–ª         Prepared:        301\n",
      "Token: –∏–∑         Prepared:        561\n",
      "Token: —É</w>      Prepared:        180\n",
      "Token: —Ç–µ–∫—Å—Ç–∞</w> Prepared:      14862\n",
      "Token: !</w>      Prepared:        269\n",
      "Token: –¢–µ         Prepared:       2146\n",
      "Token: —Å—Ç–æ        Prepared:        353\n",
      "Token: –≤—ã–π</w>    Prepared:       1154\n",
      "Token: —Ç–µ–ª–µ       Prepared:      13860\n",
      "Token: —Ñ–æ–Ω</w>    Prepared:      13568\n",
      "Token: :</w>      Prepared:        278\n",
      "Token: 8          Prepared:         19\n",
      "Token: 9          Prepared:         20\n",
      "Token: 8          Prepared:         19\n",
      "Token: 7          Prepared:         18\n",
      "Token: 6          Prepared:         17\n",
      "Token: 5          Prepared:         16\n",
      "Token: 4          Prepared:         15\n",
      "Token: 3          Prepared:         14\n",
      "Token: 2          Prepared:         13\n",
      "Token: 10</w>     Prepared:       4169\n",
      "Token: –¢–µ         Prepared:       2146\n",
      "Token: —Å—Ç–æ        Prepared:        353\n",
      "Token: –≤–∞—è</w>    Prepared:        591\n",
      "Token: –ø–æ—á        Prepared:       1289\n",
      "Token: —Ç–∞</w>     Prepared:        557\n",
      "Token: :</w>      Prepared:        278\n",
      "Token: my         Prepared:       2065\n",
      "Token: em         Prepared:        626\n",
      "Token: a          Prepared:         53\n",
      "Token: il</w>     Prepared:        880\n",
      "Token: em         Prepared:        626\n",
      "Token: a          Prepared:         53\n",
      "Token: il</w>     Prepared:        880\n",
      "Token: .</w>      Prepared:        259\n",
      "Token: co         Prepared:        689\n",
      "Token: m</w>      Prepared:        160\n",
      "Token: –¢–µ         Prepared:       2146\n",
      "Token: —Å—Ç–æ        Prepared:        353\n",
      "Token: –≤–∞—è</w>    Prepared:        591\n",
      "Token: —Å—É–º        Prepared:       9383\n",
      "Token: –º–∞</w>     Prepared:        729\n",
      "Token: –¥–µ–Ω–µ–≥</w>  Prepared:       3978\n",
      "Token: :</w>      Prepared:        278\n",
      "Token: 1          Prepared:         12\n",
      "Token: 2          Prepared:         13\n",
      "Token: 3          Prepared:         14\n",
      "Token: 4          Prepared:         15\n",
      "Token: 5          Prepared:         16\n",
      "Token: 6</w>      Prepared:        226\n"
     ]
    }
   ],
   "source": [
    "print_preprocess(encoding.tokens, encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:03.482991Z",
     "start_time": "2021-09-08T13:00:03.478278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=95, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(en_text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:03.494821Z",
     "start_time": "2021-09-08T13:00:03.485621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: H          Prepared:         31\n",
      "Token: i</w>      Prepared:        204\n",
      "Token: ,</w>      Prepared:        275\n",
      "Token: my</w>     Prepared:        975\n",
      "Token: name</w>   Prepared:       3054\n",
      "Token: is</w>     Prepared:        361\n",
      "Token: K          Prepared:         34\n",
      "Token: i          Prepared:         61\n",
      "Token: rill</w>   Prepared:       7282\n",
      "Token: Tu         Prepared:      15354\n",
      "Token: sh         Prepared:        694\n",
      "Token: in</w>     Prepared:        401\n",
      "Token: ,</w>      Prepared:        275\n",
      "Token: I</w>      Prepared:        220\n",
      "Token: was</w>    Prepared:        457\n",
      "Token: born</w>   Prepared:      11489\n",
      "Token: on</w>     Prepared:        394\n",
      "Token: 2          Prepared:         13\n",
      "Token: 6</w>      Prepared:        226\n",
      "Token: .</w>      Prepared:        259\n",
      "Token: 0          Prepared:         11\n",
      "Token: 9</w>      Prepared:        214\n",
      "Token: .</w>      Prepared:        259\n",
      "Token: 1          Prepared:         12\n",
      "Token: 9          Prepared:         20\n",
      "Token: 9          Prepared:         20\n",
      "Token: 7</w>      Prepared:        218\n",
      "Token: ,</w>      Prepared:        275\n",
      "Token: gra        Prepared:       2122\n",
      "Token: du         Prepared:       1673\n",
      "Token: ated</w>   Prepared:       1916\n",
      "Token: from</w>   Prepared:       1010\n",
      "Token: M          Prepared:         36\n",
      "Token: I          Prepared:         32\n",
      "Token: P          Prepared:         39\n",
      "Token: T</w>      Prepared:        261\n",
      "Token: .</w>      Prepared:        259\n",
      "Token: I</w>      Prepared:        220\n",
      "Token: '</w>      Prepared:        238\n",
      "Token: m</w>      Prepared:        160\n",
      "Token: trying</w> Prepared:       7552\n",
      "Token: to</w>     Prepared:        365\n",
      "Token: master</w> Prepared:      14014\n",
      "Token: a</w>      Prepared:        206\n",
      "Token: course</w> Prepared:       2869\n",
      "Token: on</w>     Prepared:        394\n",
      "Token: t          Prepared:         72\n",
      "Token: ext</w>    Prepared:       5275\n",
      "Token: analy      Prepared:      15825\n",
      "Token: sis</w>    Prepared:      23013\n",
      "Token: !</w>      Prepared:        269\n",
      "Token: T          Prepared:         43\n",
      "Token: est</w>    Prepared:        850\n",
      "Token: ph         Prepared:       3273\n",
      "Token: one</w>    Prepared:        636\n",
      "Token: :</w>      Prepared:        278\n",
      "Token: 8          Prepared:         19\n",
      "Token: 9          Prepared:         20\n",
      "Token: 8          Prepared:         19\n",
      "Token: 7          Prepared:         18\n",
      "Token: 6          Prepared:         17\n",
      "Token: 5          Prepared:         16\n",
      "Token: 4          Prepared:         15\n",
      "Token: 3          Prepared:         14\n",
      "Token: 2          Prepared:         13\n",
      "Token: 10</w>     Prepared:       4169\n",
      "Token: T          Prepared:         43\n",
      "Token: est</w>    Prepared:        850\n",
      "Token: ma         Prepared:        638\n",
      "Token: il</w>     Prepared:        880\n",
      "Token: :</w>      Prepared:        278\n",
      "Token: my         Prepared:       2065\n",
      "Token: em         Prepared:        626\n",
      "Token: a          Prepared:         53\n",
      "Token: il</w>     Prepared:        880\n",
      "Token: em         Prepared:        626\n",
      "Token: a          Prepared:         53\n",
      "Token: il</w>     Prepared:        880\n",
      "Token: .</w>      Prepared:        259\n",
      "Token: co         Prepared:        689\n",
      "Token: m</w>      Prepared:        160\n",
      "Token: T          Prepared:         43\n",
      "Token: est</w>    Prepared:        850\n",
      "Token: am         Prepared:       1022\n",
      "Token: oun        Prepared:        849\n",
      "Token: t</w>      Prepared:        183\n",
      "Token: of</w>     Prepared:        369\n",
      "Token: money</w>  Prepared:       5669\n",
      "Token: :</w>      Prepared:        278\n",
      "Token: 1          Prepared:         12\n",
      "Token: 2          Prepared:         13\n",
      "Token: 3          Prepared:         14\n",
      "Token: 4          Prepared:         15\n",
      "Token: 5          Prepared:         16\n",
      "Token: 6</w>      Prepared:        226\n"
     ]
    }
   ],
   "source": [
    "print_preprocess(encoding.tokens, encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:00:10.371740Z",
     "start_time": "2021-09-08T11:00:10.369083Z"
    }
   },
   "source": [
    "## WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:03.501527Z",
     "start_time": "2021-09-08T13:00:03.498861Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:06.807897Z",
     "start_time": "2021-09-08T13:00:03.503643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train_from_iterator(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:06.819147Z",
     "start_time": "2021-09-08T13:00:06.813864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=87, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(ru_text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:06.845120Z",
     "start_time": "2021-09-08T13:00:06.822528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: –ø—Ä–∏–≤–µ—Ç     Prepared:       5011\n",
      "Token: ,          Prepared:         12\n",
      "Token: –º–µ–Ω—è       Prepared:        643\n",
      "Token: –∑–æ–≤—É—Ç      Prepared:      14317\n",
      "Token: —Ç—É—à–∏–Ω      Prepared:       3168\n",
      "Token: –∫–∏—Ä–∏–ª–ª     Prepared:      16771\n",
      "Token: ,          Prepared:         12\n",
      "Token: —è          Prepared:         90\n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared:      19495\n",
      "Token: 2          Prepared:         17\n",
      "Token: ##6        Prepared:        162\n",
      "Token: .          Prepared:         14\n",
      "Token: 0          Prepared:         15\n",
      "Token: ##9        Prepared:        154\n",
      "Token: .          Prepared:         14\n",
      "Token: 19         Prepared:      10355\n",
      "Token: ##9        Prepared:        154\n",
      "Token: ##7        Prepared:        144\n",
      "Token: ,          Prepared:         12\n",
      "Token: –∑–∞–∫–æ–Ω      Prepared:       6972\n",
      "Token: ##—á–∏–ª      Prepared:       1727\n",
      "Token: –º          Prepared:         71\n",
      "Token: ##—Ñ        Prepared:        131\n",
      "Token: ##—Ç–∏       Prepared:        259\n",
      "Token: .          Prepared:         14\n",
      "Token: —è          Prepared:         90\n",
      "Token: –ø—ã         Prepared:       5552\n",
      "Token: ##—Ç–∞—é      Prepared:      11718\n",
      "Token: ##—Å—å       Prepared:        196\n",
      "Token: –æ—Å         Prepared:        534\n",
      "Token: ##–≤–æ–∏      Prepared:       1138\n",
      "Token: ##—Ç—å       Prepared:        182\n",
      "Token: –∫—É—Ä        Prepared:       3351\n",
      "Token: ##—Å        Prepared:         97\n",
      "Token: –ø–æ         Prepared:        167\n",
      "Token: –∞–Ω–∞        Prepared:        933\n",
      "Token: ##–ª–∏       Prepared:        215\n",
      "Token: ##–∑—É       Prepared:       1657\n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared:      14534\n",
      "Token: !          Prepared:          5\n",
      "Token: —Ç–µ—Å—Ç       Prepared:      17514\n",
      "Token: ##–æ–≤—ã      Prepared:       8365\n",
      "Token: ##–∏        Prepared:        104\n",
      "Token: —Ç–µ–ª–µ       Prepared:      12898\n",
      "Token: ##—Ñ–æ–Ω      Prepared:      17701\n",
      "Token: :          Prepared:         25\n",
      "Token: 8          Prepared:         23\n",
      "Token: ##9        Prepared:        154\n",
      "Token: ##8        Prepared:        153\n",
      "Token: ##7        Prepared:        144\n",
      "Token: ##6        Prepared:        162\n",
      "Token: ##5        Prepared:        161\n",
      "Token: ##4        Prepared:        163\n",
      "Token: ##3        Prepared:        160\n",
      "Token: ##2        Prepared:        159\n",
      "Token: ##1        Prepared:        158\n",
      "Token: ##0        Prepared:        142\n",
      "Token: —Ç–µ—Å—Ç       Prepared:      17514\n",
      "Token: ##–æ–≤–∞—è     Prepared:       9064\n",
      "Token: –ø–æ—á—Ç       Prepared:       8520\n",
      "Token: ##–∞        Prepared:        109\n",
      "Token: :          Prepared:         25\n",
      "Token: my         Prepared:        623\n",
      "Token: ##em       Prepared:        665\n",
      "Token: ##ai       Prepared:       7805\n",
      "Token: ##l        Prepared:        116\n",
      "Token: [UNK]      Prepared:          1\n",
      "Token: em         Prepared:       1941\n",
      "Token: ##ai       Prepared:       7805\n",
      "Token: ##l        Prepared:        116\n",
      "Token: .          Prepared:         14\n",
      "Token: com        Prepared:        901\n",
      "Token: —Ç–µ—Å—Ç       Prepared:      17514\n",
      "Token: ##–æ–≤–∞—è     Prepared:       9064\n",
      "Token: —Å—É–º        Prepared:       8484\n",
      "Token: ##–º–∞       Prepared:        267\n",
      "Token: –¥–µ–Ω–µ–≥      Prepared:       3731\n",
      "Token: :          Prepared:         25\n",
      "Token: 12         Prepared:       9522\n",
      "Token: ##3        Prepared:        160\n",
      "Token: ##4        Prepared:        163\n",
      "Token: ##5        Prepared:        161\n",
      "Token: ##6        Prepared:        162\n",
      "Token: [UNK]      Prepared:          1\n",
      "Token: [UNK]      Prepared:          1\n",
      "Token: [UNK]      Prepared:          1\n",
      "Token: [UNK]      Prepared:          1\n"
     ]
    }
   ],
   "source": [
    "print_preprocess(encoding.tokens, encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:06.855532Z",
     "start_time": "2021-09-08T13:00:06.848507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=93, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(en_text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:06.903624Z",
     "start_time": "2021-09-08T13:00:06.861678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: h          Prepared:         38\n",
      "Token: ##i        Prepared:        127\n",
      "Token: ,          Prepared:         12\n",
      "Token: my         Prepared:        623\n",
      "Token: name       Prepared:       2863\n",
      "Token: is         Prepared:        377\n",
      "Token: k          Prepared:         41\n",
      "Token: ##ir       Prepared:        412\n",
      "Token: ##ill      Prepared:        559\n",
      "Token: tu         Prepared:      21740\n",
      "Token: ##sh       Prepared:       2317\n",
      "Token: ##in       Prepared:        191\n",
      "Token: ,          Prepared:         12\n",
      "Token: i          Prepared:         39\n",
      "Token: was        Prepared:        324\n",
      "Token: born       Prepared:      12439\n",
      "Token: on         Prepared:        345\n",
      "Token: 2          Prepared:         17\n",
      "Token: ##6        Prepared:        162\n",
      "Token: .          Prepared:         14\n",
      "Token: 0          Prepared:         15\n",
      "Token: ##9        Prepared:        154\n",
      "Token: .          Prepared:         14\n",
      "Token: 19         Prepared:      10355\n",
      "Token: ##9        Prepared:        154\n",
      "Token: ##7        Prepared:        144\n",
      "Token: ,          Prepared:         12\n",
      "Token: gra        Prepared:       1736\n",
      "Token: ##du       Prepared:       3853\n",
      "Token: ##ated     Prepared:       1649\n",
      "Token: from       Prepared:        847\n",
      "Token: mi         Prepared:      21679\n",
      "Token: ##pt       Prepared:       1299\n",
      "Token: .          Prepared:         14\n",
      "Token: i          Prepared:         39\n",
      "Token: '          Prepared:          8\n",
      "Token: m          Prepared:         43\n",
      "Token: trying     Prepared:       7646\n",
      "Token: to         Prepared:        233\n",
      "Token: master     Prepared:       8198\n",
      "Token: a          Prepared:         31\n",
      "Token: course     Prepared:       2724\n",
      "Token: on         Prepared:        345\n",
      "Token: te         Prepared:       6280\n",
      "Token: ##xt       Prepared:       4483\n",
      "Token: analy      Prepared:      15345\n",
      "Token: ##s        Prepared:        138\n",
      "Token: ##is       Prepared:        231\n",
      "Token: !          Prepared:          5\n",
      "Token: test       Prepared:      12842\n",
      "Token: ph         Prepared:       3321\n",
      "Token: ##one      Prepared:       1413\n",
      "Token: :          Prepared:         25\n",
      "Token: 8          Prepared:         23\n",
      "Token: ##9        Prepared:        154\n",
      "Token: ##8        Prepared:        153\n",
      "Token: ##7        Prepared:        144\n",
      "Token: ##6        Prepared:        162\n",
      "Token: ##5        Prepared:        161\n",
      "Token: ##4        Prepared:        163\n",
      "Token: ##3        Prepared:        160\n",
      "Token: ##2        Prepared:        159\n",
      "Token: ##1        Prepared:        158\n",
      "Token: ##0        Prepared:        142\n",
      "Token: test       Prepared:      12842\n",
      "Token: ma         Prepared:        814\n",
      "Token: ##il       Prepared:        508\n",
      "Token: :          Prepared:         25\n",
      "Token: my         Prepared:        623\n",
      "Token: ##em       Prepared:        665\n",
      "Token: ##ai       Prepared:       7805\n",
      "Token: ##l        Prepared:        116\n",
      "Token: [UNK]      Prepared:          1\n",
      "Token: em         Prepared:       1941\n",
      "Token: ##ai       Prepared:       7805\n",
      "Token: ##l        Prepared:        116\n",
      "Token: .          Prepared:         14\n",
      "Token: com        Prepared:        901\n",
      "Token: test       Prepared:      12842\n",
      "Token: am         Prepared:        659\n",
      "Token: ##ount     Prepared:       9656\n",
      "Token: of         Prepared:        242\n",
      "Token: money      Prepared:       4901\n",
      "Token: :          Prepared:         25\n",
      "Token: 12         Prepared:       9522\n",
      "Token: ##3        Prepared:        160\n",
      "Token: ##4        Prepared:        163\n",
      "Token: ##5        Prepared:        161\n",
      "Token: ##6        Prepared:        162\n",
      "Token: [UNK]      Prepared:          1\n",
      "Token: [UNK]      Prepared:          1\n",
      "Token: [UNK]      Prepared:          1\n",
      "Token: [UNK]      Prepared:          1\n"
     ]
    }
   ],
   "source": [
    "print_preprocess(encoding.tokens, encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece (it used Unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:06.914170Z",
     "start_time": "2021-09-08T13:00:06.909875Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:10.803697Z",
     "start_time": "2021-09-08T13:00:06.917839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train_from_iterator(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:10.811314Z",
     "start_time": "2021-09-08T13:00:10.806332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=92, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(ru_text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:10.824245Z",
     "start_time": "2021-09-08T13:00:10.814667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ‚ñÅ–ü—Ä–∏       Prepared:       2048\n",
      "Token: –≤–µ—Ç,       Prepared:      10971\n",
      "Token: ‚ñÅ–º–µ–Ω—è      Prepared:        863\n",
      "Token: ‚ñÅ–∑–æ–≤—É—Ç     Prepared:      26341\n",
      "Token: ‚ñÅ–¢—É—à–∏–Ω     Prepared:       5909\n",
      "Token: ‚ñÅ–ö–∏—Ä–∏–ª     Prepared:       7430\n",
      "Token: –ª,         Prepared:       1004\n",
      "Token: —è          Prepared:        151\n",
      "Token: ‚ñÅ—Ä–æ–¥–∏      Prepared:       4325\n",
      "Token: –ª—Å—è        Prepared:        389\n",
      "Token: ‚ñÅ2         Prepared:       2880\n",
      "Token: 6          Prepared:         17\n",
      "Token: .          Prepared:         10\n",
      "Token: 0          Prepared:         11\n",
      "Token: 9          Prepared:         20\n",
      "Token: .          Prepared:         10\n",
      "Token: 1          Prepared:         12\n",
      "Token: 9          Prepared:         20\n",
      "Token: 9          Prepared:         20\n",
      "Token: 7          Prepared:         18\n",
      "Token: ,          Prepared:          8\n",
      "Token: ‚ñÅ–∑–∞–∫–æ–Ω     Prepared:       9525\n",
      "Token: —á–∏–ª        Prepared:       1947\n",
      "Token: ‚ñÅ–ú         Prepared:        342\n",
      "Token: –§          Prepared:        110\n",
      "Token: –¢          Prepared:        108\n",
      "Token: –ò          Prepared:         98\n",
      "Token: .          Prepared:         10\n",
      "Token: –Ø          Prepared:        119\n",
      "Token: ‚ñÅ–ø—ã        Prepared:       5759\n",
      "Token: —Ç–∞         Prepared:        239\n",
      "Token: —é—Å—å        Prepared:       2958\n",
      "Token: ‚ñÅ–æ—Å        Prepared:       1887\n",
      "Token: –≤–æ         Prepared:        184\n",
      "Token: –∏—Ç—å        Prepared:       2567\n",
      "Token: ‚ñÅ–∫—É—Ä       Prepared:       5870\n",
      "Token: —Å          Prepared:        137\n",
      "Token: ‚ñÅ–ø–æ        Prepared:        173\n",
      "Token: ‚ñÅ–∞–Ω        Prepared:       2616\n",
      "Token: –∞–ª–∏        Prepared:        314\n",
      "Token: –∑—É         Prepared:       2485\n",
      "Token: ‚ñÅ—Ç–µ–∫—Å—Ç–∞    Prepared:      17267\n",
      "Token: !          Prepared:          1\n",
      "Token: –¢          Prepared:        108\n",
      "Token: –µ          Prepared:        125\n",
      "Token: —Å—Ç–æ        Prepared:        296\n",
      "Token: –≤—ã–π        Prepared:       1248\n",
      "Token: ‚ñÅ—Ç–µ–ª–µ      Prepared:      15247\n",
      "Token: —Ñ–æ–Ω        Prepared:      27121\n",
      "Token: :          Prepared:         21\n",
      "Token: ‚ñÅ8         Prepared:       5978\n",
      "Token: 9          Prepared:         20\n",
      "Token: 8          Prepared:         19\n",
      "Token: 7          Prepared:         18\n",
      "Token: 6          Prepared:         17\n",
      "Token: 5          Prepared:         16\n",
      "Token: 4          Prepared:         15\n",
      "Token: 3          Prepared:         14\n",
      "Token: 2          Prepared:         13\n",
      "Token: 1          Prepared:         12\n",
      "Token: 0          Prepared:         11\n",
      "Token: –¢          Prepared:        108\n",
      "Token: –µ          Prepared:        125\n",
      "Token: —Å—Ç–æ        Prepared:        296\n",
      "Token: –≤–∞—è        Prepared:        475\n",
      "Token: ‚ñÅ–ø–æ—á       Prepared:        886\n",
      "Token: —Ç–∞         Prepared:        239\n",
      "Token: :          Prepared:         21\n",
      "Token: ‚ñÅmy        Prepared:        792\n",
      "Token: em         Prepared:        697\n",
      "Token: a          Prepared:         53\n",
      "Token: ile        Prepared:       2085\n",
      "Token: m          Prepared:         65\n",
      "Token: ail        Prepared:       9388\n",
      "Token: .          Prepared:         10\n",
      "Token: c          Prepared:         55\n",
      "Token: om         Prepared:        306\n",
      "Token: –¢          Prepared:        108\n",
      "Token: –µ          Prepared:        125\n",
      "Token: —Å—Ç–æ        Prepared:        296\n",
      "Token: –≤–∞—è        Prepared:        475\n",
      "Token: ‚ñÅ—Å—É–º       Prepared:       9709\n",
      "Token: –º–∞         Prepared:        299\n",
      "Token: ‚ñÅ–¥–µ–Ω–µ–≥     Prepared:       6180\n",
      "Token: :          Prepared:         21\n",
      "Token: ‚ñÅ12        Prepared:      11485\n",
      "Token: 3          Prepared:         14\n",
      "Token: 4          Prepared:         15\n",
      "Token: 5          Prepared:         16\n",
      "Token: 6          Prepared:         17\n",
      "Token: ‚ñÅ          Prepared:        157\n",
      "Token: ‚ñÅ          Prepared:        157\n"
     ]
    }
   ],
   "source": [
    "print_preprocess(encoding.tokens, encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:10.838954Z",
     "start_time": "2021-09-08T13:00:10.831174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=97, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(en_text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:10.852505Z",
     "start_time": "2021-09-08T13:00:10.842014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ‚ñÅH         Prepared:        354\n",
      "Token: i,         Prepared:       2652\n",
      "Token: ‚ñÅmy        Prepared:        792\n",
      "Token: ‚ñÅname      Prepared:       4297\n",
      "Token: ‚ñÅis        Prepared:        427\n",
      "Token: ‚ñÅK         Prepared:       4068\n",
      "Token: i          Prepared:         61\n",
      "Token: rill       Prepared:       8024\n",
      "Token: ‚ñÅT         Prepared:        418\n",
      "Token: us         Prepared:       1081\n",
      "Token: h          Prepared:         60\n",
      "Token: in,        Prepared:       3775\n",
      "Token: I          Prepared:         32\n",
      "Token: ‚ñÅwas       Prepared:        385\n",
      "Token: ‚ñÅborn      Prepared:      12169\n",
      "Token: ‚ñÅon        Prepared:        425\n",
      "Token: ‚ñÅ2         Prepared:       2880\n",
      "Token: 6          Prepared:         17\n",
      "Token: .          Prepared:         10\n",
      "Token: 0          Prepared:         11\n",
      "Token: 9          Prepared:         20\n",
      "Token: .          Prepared:         10\n",
      "Token: 1          Prepared:         12\n",
      "Token: 9          Prepared:         20\n",
      "Token: 9          Prepared:         20\n",
      "Token: 7          Prepared:         18\n",
      "Token: ,          Prepared:          8\n",
      "Token: ‚ñÅgra       Prepared:       1921\n",
      "Token: du         Prepared:       3914\n",
      "Token: ated       Prepared:       2273\n",
      "Token: ‚ñÅfrom      Prepared:        966\n",
      "Token: ‚ñÅM         Prepared:        695\n",
      "Token: I          Prepared:         32\n",
      "Token: P          Prepared:         39\n",
      "Token: T          Prepared:         43\n",
      "Token: .          Prepared:         10\n",
      "Token: I          Prepared:         32\n",
      "Token: '          Prepared:          4\n",
      "Token: m          Prepared:         65\n",
      "Token: ‚ñÅtrying    Prepared:       8732\n",
      "Token: ‚ñÅto        Prepared:        265\n",
      "Token: ‚ñÅmaster    Prepared:       8775\n",
      "Token: ‚ñÅa         Prepared:        175\n",
      "Token: ‚ñÅcourse    Prepared:       4141\n",
      "Token: ‚ñÅon        Prepared:        425\n",
      "Token: ‚ñÅte        Prepared:      11022\n",
      "Token: xt         Prepared:       4591\n",
      "Token: ‚ñÅanaly     Prepared:      18288\n",
      "Token: s          Prepared:         71\n",
      "Token: is         Prepared:        226\n",
      "Token: !          Prepared:          1\n",
      "Token: T          Prepared:         43\n",
      "Token: est        Prepared:        691\n",
      "Token: ‚ñÅph        Prepared:       5793\n",
      "Token: one        Prepared:       1344\n",
      "Token: :          Prepared:         21\n",
      "Token: ‚ñÅ8         Prepared:       5978\n",
      "Token: 9          Prepared:         20\n",
      "Token: 8          Prepared:         19\n",
      "Token: 7          Prepared:         18\n",
      "Token: 6          Prepared:         17\n",
      "Token: 5          Prepared:         16\n",
      "Token: 4          Prepared:         15\n",
      "Token: 3          Prepared:         14\n",
      "Token: 2          Prepared:         13\n",
      "Token: 1          Prepared:         12\n",
      "Token: 0          Prepared:         11\n",
      "Token: T          Prepared:         43\n",
      "Token: est        Prepared:        691\n",
      "Token: ‚ñÅma        Prepared:       1029\n",
      "Token: il         Prepared:        528\n",
      "Token: :          Prepared:         21\n",
      "Token: ‚ñÅmy        Prepared:        792\n",
      "Token: em         Prepared:        697\n",
      "Token: a          Prepared:         53\n",
      "Token: ile        Prepared:       2085\n",
      "Token: m          Prepared:         65\n",
      "Token: ail        Prepared:       9388\n",
      "Token: .          Prepared:         10\n",
      "Token: c          Prepared:         55\n",
      "Token: om         Prepared:        306\n",
      "Token: T          Prepared:         43\n",
      "Token: est        Prepared:        691\n",
      "Token: ‚ñÅam        Prepared:        801\n",
      "Token: ount       Prepared:       9490\n",
      "Token: ‚ñÅof        Prepared:        271\n",
      "Token: ‚ñÅmoney     Prepared:       7855\n",
      "Token: :          Prepared:         21\n",
      "Token: ‚ñÅ12        Prepared:      11485\n",
      "Token: 3          Prepared:         14\n",
      "Token: 4          Prepared:         15\n",
      "Token: 5          Prepared:         16\n",
      "Token: 6          Prepared:         17\n",
      "Token: ‚ñÅ          Prepared:        157\n",
      "Token: ‚ñÅ          Prepared:        157\n",
      "Token: ‚ñÅ          Prepared:        157\n",
      "Token: ‚ñÅ          Prepared:        157\n"
     ]
    }
   ],
   "source": [
    "print_preprocess(encoding.tokens, encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:19.627566Z",
     "start_time": "2021-09-08T13:00:10.857293Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:19.638101Z",
     "start_time": "2021-09-08T13:00:19.630297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS]      Prepared:        101\n",
      "Token: –ø—Ä–∏–≤–µ—Ç     Prepared:      26856\n",
      "Token: ,          Prepared:        128\n",
      "Token: –º–µ–Ω—è       Prepared:      14198\n",
      "Token: –∑–æ–≤—É—Ç      Prepared:      39327\n",
      "Token: —Ç—É—à–∏       Prepared:     115235\n",
      "Token: ##–Ω        Prepared:        858\n",
      "Token: –∫–∏—Ä–∏–ª–ª     Prepared:      75305\n",
      "Token: ,          Prepared:        128\n",
      "Token: —è          Prepared:        877\n",
      "Token: —Ä–æ–¥–∏–ª—Å—è    Prepared:       9551\n",
      "Token: 26         Prepared:       7085\n",
      "Token: .          Prepared:        132\n",
      "Token: 09         Prepared:      11547\n",
      "Token: .          Prepared:        132\n",
      "Token: 1997       Prepared:      10900\n",
      "Token: ,          Prepared:        128\n",
      "Token: –∑–∞–∫–æ–Ω—á–∏–ª   Prepared:      19305\n",
      "Token: –º          Prepared:        865\n",
      "Token: ##—Ñ—Ç       Prepared:      25037\n",
      "Token: ##–∏        Prepared:        852\n",
      "Token: .          Prepared:        132\n",
      "Token: —è          Prepared:        877\n",
      "Token: –ø—ã—Ç–∞       Prepared:      21666\n",
      "Token: ##—é—Å—å      Prepared:      29627\n",
      "Token: –æ—Å–≤–æ–∏—Ç—å    Prepared:      82042\n",
      "Token: –∫—É—Ä—Å       Prepared:      12986\n",
      "Token: –ø–æ         Prepared:       1516\n",
      "Token: –∞–Ω–∞–ª–∏–∑—É    Prepared:      49382\n",
      "Token: —Ç–µ–∫—Å—Ç–∞     Prepared:      22574\n",
      "Token: !          Prepared:        106\n",
      "Token: —Ç–µ—Å—Ç–æ–≤     Prepared:      50871\n",
      "Token: ##—ã        Prepared:        880\n",
      "Token: ##–∏        Prepared:        852\n",
      "Token: —Ç–µ–ª–µ—Ñ–æ–Ω    Prepared:      17469\n",
      "Token: :          Prepared:        156\n",
      "Token: 898        Prepared:      86285\n",
      "Token: ##76       Prepared:      53737\n",
      "Token: ##54       Prepared:      36695\n",
      "Token: ##32       Prepared:      34204\n",
      "Token: ##10       Prepared:      13368\n",
      "Token: —Ç–µ—Å—Ç–æ–≤     Prepared:      50871\n",
      "Token: ##–∞—è       Prepared:       1637\n",
      "Token: –ø–æ—á—Ç–∞      Prepared:      42241\n",
      "Token: :          Prepared:        156\n",
      "Token: my         Prepared:      15639\n",
      "Token: ##ema      Prepared:      18601\n",
      "Token: ##il       Prepared:       6515\n",
      "Token: @          Prepared:        168\n",
      "Token: em         Prepared:      10778\n",
      "Token: ##ail      Prepared:      14864\n",
      "Token: .          Prepared:        132\n",
      "Token: com        Prepared:      10724\n",
      "Token: —Ç–µ—Å—Ç–æ–≤     Prepared:      50871\n",
      "Token: ##–∞—è       Prepared:       1637\n",
      "Token: —Å—É–º–º–∞      Prepared:      19743\n",
      "Token: –¥–µ–Ω–µ–≥      Prepared:      16244\n",
      "Token: :          Prepared:        156\n",
      "Token: 1234       Prepared:      83261\n",
      "Token: ##56       Prepared:      40340\n",
      "Token: $          Prepared:        112\n",
      "Token: [UNK]      Prepared:        100\n",
      "Token: [UNK]      Prepared:        100\n",
      "Token: [UNK]      Prepared:        100\n",
      "Token: [SEP]      Prepared:        102\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.encode(ru_text)\n",
    "tokens = [tokenizer._convert_id_to_token(index) for index in indexes]\n",
    "\n",
    "print_preprocess(tokens, indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CleanUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:19.779759Z",
     "start_time": "2021-09-08T13:00:19.641250Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "def clean_text(\n",
    "    text,\n",
    "    strip=True,\n",
    "    punctuation=True,\n",
    "    lower=True,\n",
    "):\n",
    "    if strip:\n",
    "        text = text.strip()\n",
    "\n",
    "    if punctuation:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('\\t', ' ')\n",
    "    \n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text_from_xml(xml_text):\n",
    "    return BS(xml_text, \"xml\").get_text(separator='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:19.787677Z",
     "start_time": "2021-09-08T13:00:19.782984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ø—Ä–∏–≤–µ—Ç –º–µ–Ω—è –∑–æ–≤—É—Ç —Ç—É—à–∏–Ω –∫–∏—Ä–∏–ª–ª —è —Ä–æ–¥–∏–ª—Å—è 26091997 –∑–∞–∫–æ–Ω—á–∏–ª –º—Ñ—Ç–∏ —è –ø—ã—Ç–∞—é—Å—å –æ—Å–≤–æ–∏—Ç—å –∫—É—Ä—Å –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–∞ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω 89876543210 —Ç–µ—Å—Ç–æ–≤–∞—è –ø–æ—á—Ç–∞ myemailemailcom —Ç–µ—Å—Ç–æ–≤–∞—è —Å—É–º–º–∞ –¥–µ–Ω–µ–≥ 123456 üòé üî• ‚úåÔ∏è'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(ru_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:19.795340Z",
     "start_time": "2021-09-08T13:00:19.790016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi my name is kirill tushin i was born on 26091997 graduated from mipt im trying to master a course on text analysis test phone 89876543210 test mail myemailemailcom test amount of money 123456 üòé  üî•  ‚úåÔ∏è'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(en_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace text with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:20.378730Z",
     "start_time": "2021-09-08T13:00:19.798275Z"
    }
   },
   "outputs": [],
   "source": [
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:20.387890Z",
     "start_time": "2021-09-08T13:00:20.380426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ø—Ä–∏–≤–µ—Ç –º–µ–Ω—è –∑–æ–≤—É—Ç —Ç—É—à–∏–Ω –∫–∏—Ä–∏–ª–ª\\n—è —Ä–æ–¥–∏–ª—Å—è 00000000 –∑–∞–∫–æ–Ω—á–∏–ª –º—Ñ—Ç–∏\\n—è –ø—ã—Ç–∞—é—Å—å –æ—Å–≤–æ–∏—Ç—å –∫—É—Ä—Å –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–∞\\n—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω <phone>\\n—Ç–µ—Å—Ç–æ–≤–∞—è –ø–æ—á—Ç–∞ <email>\\n—Ç–µ—Å—Ç–æ–≤–∞—è —Å—É–º–º–∞ –¥–µ–Ω–µ–≥ 000000<cur>\\nÔ∏è'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(\n",
    "    text=ru_text,\n",
    "    to_ascii=False,\n",
    "    no_urls=True,\n",
    "    no_emails=True,\n",
    "    no_phone_numbers=True,\n",
    "    no_digits=True,\n",
    "    no_currency_symbols=True,\n",
    "    no_punct=True,\n",
    "    no_emoji=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:00:20.395608Z",
     "start_time": "2021-09-08T13:00:20.389850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi my name is kirill tushin\\ni was born on 00000000 graduated from mipt\\nim trying to master a course on text analysis\\ntest phone <phone>\\ntest mail <email>\\ntest amount of money 000000<cur>\\nÔ∏è'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(\n",
    "    text=en_text,\n",
    "    to_ascii=False,\n",
    "    no_urls=True,\n",
    "    no_emails=True,\n",
    "    no_phone_numbers=True,\n",
    "    no_digits=True,\n",
    "    no_currency_symbols=True,\n",
    "    no_punct=True,\n",
    "    no_emoji=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "663.478271484375px",
    "left": "22px",
    "top": "0px",
    "width": "272.43206787109375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
